{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4a98079-4d16-4e20-b9a2-0105f08ae066",
   "metadata": {},
   "source": [
    "The provided code defines a Python function `vertical_transect` that processes meteorological data, generates visualizations, and saves them as PNG images. The code appears to be quite extensive, combining data processing, visualization generation, and file saving into one function. Here's a description or summary of what the code does:\n",
    "\n",
    "---\n",
    "\n",
    "The `vertical_transect` function processes meteorological data collected by a KAZR (K-band zenith radar) and interpolated sonde measurements. It generates a series of vertical transect visualizations and saves them as PNG images. The processed data includes information such as reflectivity, Doppler velocity, spectral width, liquid water path, precipitation rate, and temperature.\n",
    "\n",
    "The function begins by specifying paths to data directories and reading relevant data files in NetCDF format. It then processes the data based on a list of formatted date ranges and a reference CAO (Central Analysis Office) date. For each date range in the list:\n",
    "\n",
    "1. Subsets of data are extracted from the KAZR and sonde datasets based on the specified date range.\n",
    "2. Various meteorological variables such as reflectivity, Doppler velocity, spectral width, liquid water path, precipitation rate, and temperature are extracted from the datasets.\n",
    "3. Gaussian smoothing is applied to the reflectivity, Doppler velocity, and spectral width data.\n",
    "4. Processed data variables are stored in separate lists for later use.\n",
    "\n",
    "After processing the data for all date ranges, the function proceeds to generate visualizations for each date range:\n",
    "\n",
    "1. A set of subplots is created for each variable, and labels are added to each subplot.\n",
    "2. Different color maps and color scales are applied to different variables for meaningful visualization.\n",
    "3. The visualizations include data like reflectivity, Doppler velocity, spectral width, and combined liquid water path and precipitation rate.\n",
    "4. Custom datetime formatting is applied to the x-axis labels, and tick positions are adjusted for clarity.\n",
    "5. Each generated visualization is saved as a PNG image file in a specified directory structure.\n",
    "\n",
    "Finally, the function prints a success message indicating the completion of data processing and visualization generation.\n",
    "\n",
    "---\n",
    "\n",
    "Please note that the above description is a general overview of the code's functionality based on the provided code snippet. If there are specific details or nuances in your implementation that you'd like to emphasize or include in the description, feel free to make adjustments accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91d36f51-d5a8-47c2-82b9-3390294c805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "import matplotlib.dates as mdates  # Importing module for date formatting\n",
    "import matplotlib.ticker as ticker  # Importing ticker for customizing colorbar ticks\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58581d1-da59-4cd9-8569-377463941ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc28ff2d-c103-4c8c-a878-987b8827ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_transect(formatted_date:list, CAO_date:str):\n",
    "    \n",
    "    # path to data\n",
    "    path1 = \"/glade/work/noteng/masters-research/data/anx-data/otengn1/240475/\"\n",
    "    # kazr data\n",
    "    data1 = \"KAZR/anxarsclkazr1kolliasM1/\"\n",
    "    # get upper temperature from this\n",
    "    data2 = \"interpsonde/\"\n",
    "    # reading all data\n",
    "    data = glob.glob(f\"{path1}{data1}*.nc\")\n",
    "    data_sonde = glob.glob(f\"{path1}{data2}*.nc\")\n",
    "\n",
    "    ds = xr.open_mfdataset(data)\n",
    "    ds1 = xr.open_mfdataset(data_sonde)\n",
    "\n",
    "    ds_subsets = []; ds1_subsets = []\n",
    "\n",
    "    reflectivitys = []; doppler_velocitys = []; spectral_widths = []; lwps = []; precips = []; temps = []\n",
    "\n",
    "    # Apply Gaussian smoothing\n",
    "    smoothed_reflectivitys = []; smoothed_doppler_velocitys = []; smoothed_spectral_widths = []\n",
    "\n",
    "    # getting the data in formatted way as in DataArray\n",
    "    for entry in formatted_date:\n",
    "        name, start_time, end_time = entry\n",
    "        # kazr\n",
    "        ds_subset = ds.sel(time=slice(start_time, end_time))\n",
    "        # interpolate sonde(get temperature variable from this)\n",
    "        ds1_subset = ds1.sel(time=slice(start_time, end_time))\n",
    "\n",
    "        # gettting variables from subset\n",
    "        reflectivity = ds_subset['reflectivity_best_estimate']\n",
    "        doppler_velocity = ds_subset['mean_doppler_velocity']\n",
    "        spectral_width = ds_subset['spectral_width']\n",
    "        lwp = ds_subset['mwr_lwp']\n",
    "        precip = ds_subset['precip_mean']\n",
    "        temp = ds1_subset['temp']\n",
    "\n",
    "        # Apply Gaussian smoothing\n",
    "        smoothed_reflectivity = gaussian_filter(reflectivity, sigma=0)\n",
    "        smoothed_doppler_velocity = gaussian_filter(doppler_velocity, sigma=0)\n",
    "        smoothed_spectral_width = gaussian_filter(spectral_width, sigma=0)\n",
    "\n",
    "        ##################\n",
    "        # append new results(variables)\n",
    "        ds_subsets.append(ds_subset)\n",
    "        ds1_subsets.append(ds1_subset)\n",
    "\n",
    "        reflectivitys.append(reflectivity)\n",
    "        doppler_velocitys.append(doppler_velocity)\n",
    "        spectral_widths.append(spectral_width)\n",
    "        lwps.append(lwp)\n",
    "        precips.append(precip)\n",
    "        temps.append(temp)\n",
    "\n",
    "        smoothed_reflectivitys.append(smoothed_reflectivity)\n",
    "        smoothed_doppler_velocitys.append(smoothed_doppler_velocity)\n",
    "        smoothed_spectral_widths.append(smoothed_spectral_width)\n",
    "        \n",
    "###################################### plotting code\n",
    "    for i in range(0, len(formatted_date)):\n",
    "    # for i in range(0, 2):\n",
    "\n",
    "        # fig, ax = plt.subplots(figsize=(10, 15), nrows=4, ncols=1, sharex=True)\n",
    "        fig, ax = plt.subplots(figsize=(8, 15), nrows=4, ncols=1, sharex=True)\n",
    "        ax = ax.flatten()\n",
    "        \n",
    "        \n",
    "        # text = \"Example Text\"\n",
    "        # ax[j].text(0.5, 2.05, j+1, fontsize=38, transform=ax[i].transAxes, ha='center', fontweight='bold')\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "        # adding labels to each subplot\n",
    "        for j, v in enumerate(ax):\n",
    "            labels = [\"(a)\", \"(b)\", \"(c)\", \"(d)\"]\n",
    "            # Add text to the subplots\n",
    "            ax[j].text(0.004, 0.97, labels[j], transform=ax[j].transAxes,\n",
    "            fontsize=18, va='top', ha='left', fontweight='bold') \n",
    "            \n",
    "\n",
    "        ### reflectivity\n",
    "         # if i == 1:\n",
    "       \n",
    "        pcm = ax[0].pcolormesh(ds_subsets[i]['time'], reflectivitys[i]['height']/1000.0, smoothed_reflectivitys[i].T, \n",
    "        cmap='nipy_spectral', vmin=-30, vmax=25, zorder=5)\n",
    "        \n",
    "\n",
    "        # temperature\n",
    "        cntr = ax[0].contour(temps[i]['time'][::2], temps[i]['height'][::2], np.transpose(temps[i].values)[::2, ::2], \n",
    "        np.round(np.arange(-100,320,10)), colors='k', linewidths=0.8, linestyles =\"dashed\", zorder=6)\n",
    "        plt.clabel(cntr,inline=1, inline_spacing=8, fontsize=20, fmt='%i',manual=False)\n",
    "        \n",
    "        # add id labels\n",
    "        ax[0].text(0.5, 1.1, f\"ID-{i+1}\", fontsize=35, transform=ax[0].transAxes, ha='center')\n",
    "            \n",
    "        \n",
    "\n",
    "        ax[0].set_ylim(0, 7)\n",
    "        # Set y-axis ticks\n",
    "        ax[0].set_yticks(np.arange(1, 8))\n",
    "        ax[0].tick_params(axis='y', labelsize=15)\n",
    "        ax[0].set_ylabel('Height [km]', color='black', fontsize=15)  # Label for the twin y-axis\n",
    "        cbar = fig.colorbar(pcm, ax=ax[0])\n",
    "        cbar.set_label('Reflectivity [dBZ]', fontsize=15)\n",
    "        # Set colorbar ticks at an interval of 10 (adjust as needed)\n",
    "        tick_locator = ticker.MultipleLocator(base=10)\n",
    "        cbar.locator = tick_locator\n",
    "        cbar.update_ticks()\n",
    "        \n",
    "\n",
    "        ### doppler velocity\n",
    "        # if i == 1:\n",
    "        # pcm = ax[1].pcolormesh(ds_subsets[i]['time'], reflectivitys[i]['height']/1000.0, smoothed_doppler_velocitys[i].T,\n",
    "        # cmap='bwr', vmin=-3, vmax=1, zorder=5)\n",
    "        pcm = ax[1].pcolormesh(ds_subsets[i]['time'], reflectivitys[i]['height']/1000.0, smoothed_doppler_velocitys[i].T,\n",
    "        cmap='bwr', vmin=-3, vmax=1, zorder=5)\n",
    "        ax[1].set_ylim(0, 7)\n",
    "        # Set y-axis ticks\n",
    "        ax[1].set_yticks(np.arange(1, 8))\n",
    "        ax[1].tick_params(axis='y', labelsize=15)\n",
    "        ax[1].set_ylabel('Height [km]', color='black', fontsize=15)  # Label for the twin y-axis\n",
    "        cbar = fig.colorbar(pcm, ax=ax[1], extend='both')\n",
    "        cbar.set_label(\"Doppler\\nVelocity [m s$^{-1}$]\", fontsize=15)\n",
    "\n",
    "        ### spectral width\n",
    "        # if i == 2:\n",
    "        pcm = ax[2].pcolormesh(ds_subsets[i]['time'], reflectivitys[i]['height']/1000.0, smoothed_spectral_widths[i].T,\n",
    "                               cmap='Blues', vmin=0, vmax=1, zorder=5)\n",
    "        ax[2].set_ylim(0, 7)\n",
    "        # Set y-axis ticks\n",
    "        ax[2].set_yticks(np.arange(1, 8))\n",
    "        ax[2].tick_params(axis='y', labelsize=15)\n",
    "        ax[2].set_ylabel('Height [km]', color='black', fontsize=15)  # Label for the twin y-axis\n",
    "        cbar = fig.colorbar(pcm, ax=ax[2], extend='max')\n",
    "        cbar.set_label(\"Spectral\\nWidth [m s$^{-1}$]\", fontsize=15)\n",
    "\n",
    "\n",
    "        ### liquid water path and precipitation rate\n",
    "        # if i == 3:\n",
    "        pcm = ax[3].scatter(lwps[i]['time'], np.divide(lwps[i].values, 1000), color='blue', zorder=5)\n",
    "        ax[3].set_yticks(np.arange(0, round(np.nanmax(np.divide(lwps[i].values, 1000)))+0.1, 0.2))\n",
    "        ax[3].spines['left'].set_color('blue')\n",
    "        ax[3].tick_params(axis='y', colors='blue', labelsize=15)\n",
    "        ax[3].set_ylabel(\"LWP [kg m$^{-2}$]\", c = \"blue\", fontsize=15)\n",
    "        cbar = fig.colorbar(pcm, ax=ax[3], extend='neither')\n",
    "        # set colorbar to invisible\n",
    "        cbar.ax.set_visible(False)\n",
    "        ax[3].grid(axis=\"y\", ls = '--', c = \"blue\",alpha = 0.5)\n",
    "        #####################\n",
    "        # ax[3].set_xlim(0, np.nanmax(np.divide(lwps[i].values, 1000)))\n",
    "        # ax[3].set_xticks(np.arange(1, (lwps[i].values/1000), 0.2)\n",
    "        ax[3].set_ylim(0, 2)\n",
    "        ax[3].set_yticks(np.arange(0, 2.5, 0.5))\n",
    "        # print(np.nanmin(lwps[i]/1000))\n",
    "\n",
    "        # Create a twin y-axis for ax[2]\n",
    "        ax31 = ax[3].twinx()\n",
    "        ax31.scatter(lwps[i]['time'], precips[i], color='black')\n",
    "        ax31.spines['right'].set_color('black')\n",
    "        ax31.tick_params(axis='y', colors='black', labelsize=15)\n",
    "        ax31.set_ylabel('Precip Rate [mm/hr]', color='black', fontsize=15)  # Label for the twin y-axis\n",
    "        ax31.set_yticks(np.arange(0, round(np.max(precips[i].values))+0.1))\n",
    "        ax31.grid(axis=\"y\", ls = '--', c = \"black\",alpha = 0.5)\n",
    "        ax31.set_ylim(0, 2)\n",
    "        ax31.set_yticks(np.arange(0, 2.5, 0.5))\n",
    "\n",
    "\n",
    "        # Custom datetime format\n",
    "        custom_date_format = \"%H:%M UTC\\n%d %B %Y\"  # Format: 04:00 UTC\\n13 March 2020\n",
    "        # Formatting the time labels\n",
    "        ax[len(ax)-1].xaxis.set_major_formatter(mdates.DateFormatter(custom_date_format))\n",
    "        # Including the first-date and last-date on the plot\n",
    "        ax[len(ax)-1].set_xlim([ds_subsets[i]['time'].min(), ds_subsets[i]['time'].max()])\n",
    "        # Set custom tick positions for the x-axis\n",
    "        # num_ticks = 6  # Number of tick labels\n",
    "        num_ticks = 5  # Number of tick labels\n",
    "        tick_positions = np.linspace(0, len(ds_subsets[i]['time']) - 1, num_ticks, dtype=int)\n",
    "        ax[len(ax)-1].set_xticks(ds_subsets[i]['time'][tick_positions])\n",
    "\n",
    "        # Generate custom tick labels including the first and last dates\n",
    "        tick_labels = [ds_subsets[i]['time'][pos].dt.strftime(custom_date_format).values for pos in tick_positions]\n",
    "        tick_labels[0] = ds_subsets[i]['time'].min().dt.strftime(custom_date_format).values  # First date\n",
    "        tick_labels[-1] = ds_subsets[i]['time'].max().dt.strftime(custom_date_format).values  # Last date\n",
    "        ax[len(ax)-1].set_xticklabels(tick_labels ,rotation=0);  # You can adjust fontsize and rotation\n",
    "        \n",
    "        \n",
    "        # convert datetime from '%Y-%m-%dT%H:%M:%S.%f' to '%Y-%m-%d %H:%M:%S' format\n",
    "        timestamp_start = str(ds_subsets[i]['time'].values[0]) # start time\n",
    "        timestamp_end = str(ds_subsets[i]['time'].values[-1])  # end time\n",
    "        \n",
    "        timestamp_without_nanoseconds = timestamp_start[:-3]  # Remove last three digits\n",
    "        timestamp_without_nanoseconds1 = timestamp_end[:-3]  # Remove last three digits\n",
    "        \n",
    "        timestamp_out_start = datetime.strptime(timestamp_without_nanoseconds, '%Y-%m-%dT%H:%M:%S.%f').strftime('%Y%m%d-%H:%M:%S')\n",
    "        timestamp_out_end = datetime.strptime(timestamp_without_nanoseconds1, '%Y-%m-%dT%H:%M:%S.%f').strftime('%Y%m%d-%H:%M:%S')\n",
    "        \n",
    "\n",
    "        directory = f'Open-cells-Figures-Final/t_series/{CAO_date}'\n",
    "        file_name = f\"{timestamp_out_start}_{timestamp_out_end}_ID{i+1}.png\"\n",
    "\n",
    "\n",
    "         # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(f\"Directory '{directory}' created successfully.\")\n",
    "\n",
    "        dirr = sorted(os.listdir(directory))\n",
    "        if file_name in dirr:\n",
    "            # print(f'{dt[i]}.png already exist')\n",
    "            pass\n",
    "            plt.close()\n",
    "        else:\n",
    "            fig.savefig(os.path.join(directory, file_name), dpi=500)\n",
    "            print(f\"Figures in {file_name} executed!\")\n",
    "            plt.close()\n",
    "    print(f'\\U0001f600\\U0001f600\\U0001f600\\U0001f600IDENTIFIED ALL {i+1} CELLS IDENTIFIED SUCCESSFULLY!!!\\U0001f600\\U0001f600\\U0001f600\\U0001f600')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b558afe-9273-474f-9bc4-e8ccdfdead0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, '2020-03-13 08:12:28', '2020-03-13 08:34:48'),\n",
    "    (2, '2020-03-13 08:32:08', '2020-03-13 09:06:36'),\n",
    "    (3, '2020-03-13 09:12:48', '2020-03-13 09:31:00'),\n",
    "    (4, '2020-03-13 09:29:36', '2020-03-13 09:58:16'),\n",
    "    (5, '2020-03-13 10:00:12', '2020-03-13 10:11:56'),\n",
    "    (6, '2020-03-13 10:14:32', '2020-03-13 10:29:00'),\n",
    "    (7, '2020-03-13 10:22:40', '2020-03-13 10:46:56'),\n",
    "    (8, '2020-03-13 10:42:32', '2020-03-13 10:59:04'),\n",
    "    (9, '2020-03-13 11:00:32', '2020-03-13 11:26:20'),\n",
    "    (10, '2020-03-13 11:23:32', '2020-03-13 11:32:28'),\n",
    "    (11, '2020-03-13 11:30:44', '2020-03-13 11:51:52'),\n",
    "    (12, '2020-03-13 11:48:08', '2020-03-13 12:09:04'),\n",
    "    (13, '2020-03-13 12:03:08', '2020-03-13 12:15:00'),\n",
    "    (14, '2020-03-13 12:12:24', '2020-03-13 12:38:04'),\n",
    "    (15, '2020-03-13 12:32:48', '2020-03-13 12:56:36'),\n",
    "    (16, '2020-03-13 12:52:56', '2020-03-13 13:14:32'),\n",
    "    (17, '2020-03-13 13:53:00', '2020-03-13 14:05:16'),\n",
    "    (18, '2020-03-13 14:03:28', '2020-03-13 14:31:20'),\n",
    "    (19, '2020-03-13 15:09:36', '2020-03-13 15:19:08'),\n",
    "    (20, '2020-03-13 15:17:16', '2020-03-13 15:45:20'),\n",
    "    (21, '2020-03-13 15:54:36', '2020-03-13 15:58:08'),\n",
    "    (22, '2020-03-13 15:58:12', '2020-03-13 16:02:36'),\n",
    "    (23, '2020-03-13 15:48:00', '2020-03-13 16:12:44'),\n",
    "    (24, '2020-03-13 16:12:00', '2020-03-13 16:33:44'),\n",
    "    (25, '2020-03-13 16:29:40', '2020-03-13 16:38:20'),\n",
    "    (26, '2020-03-13 16:44:24', '2020-03-13 16:57:56'),\n",
    "    (27, '2020-03-13 16:53:20', '2020-03-13 17:13:32'),\n",
    "    (28, '2020-03-13 17:12:56', '2020-03-13 17:22:56'),\n",
    "    (29, '2020-03-13 17:13:36', '2020-03-13 17:45:48'),\n",
    "    (30, '2020-03-13 18:10:04', '2020-03-13 18:28:04'),\n",
    "    (31, '2020-03-13 18:27:28', '2020-03-13 18:50:04'),\n",
    "    (32, '2020-03-13 18:47:08', '2020-03-13 18:54:48'),\n",
    "    (33, '2020-03-13 18:54:04', '2020-03-13 18:57:08'),\n",
    "    (34, '2020-03-13 19:15:32', '2020-03-13 19:54:00'),\n",
    "    (35, '2020-03-13 19:47:12', '2020-03-13 20:15:52'),\n",
    "    (36, '2020-03-13 20:14:12', '2020-03-13 20:32:32'),\n",
    "    (37, '2020-03-13 20:26:44', '2020-03-13 20:48:12'),\n",
    "    (38, '2020-03-13 20:50:16', '2020-03-13 20:59:28'),\n",
    "    (39, '2020-03-13 20:48:36', '2020-03-13 21:19:56'),\n",
    "    (40, '2020-03-13 21:07:00', '2020-03-13 21:11:04'),\n",
    "    (41, '2020-03-13 21:16:00', '2020-03-13 21:21:40'),\n",
    "    (42, '2020-03-13 21:27:40', '2020-03-13 21:50:48'),\n",
    "    (43, '2020-03-13 21:48:20', '2020-03-13 21:59:08'),\n",
    "    (44, '2020-03-13 21:57:40', '2020-03-13 22:13:00'),\n",
    "    (45, '2020-03-13 22:28:24', '2020-03-13 22:39:36'),\n",
    "    (46, '2020-03-13 22:38:08', '2020-03-13 23:00:00'),\n",
    "    (47, '2020-03-13 23:38:56', '2020-03-14 00:16:28'),\n",
    "    (48, '2020-03-14 00:13:24', '2020-03-14 00:30:44'),\n",
    "    (49, '2020-03-14 01:06:40', '2020-03-14 01:24:08'),\n",
    "    (50, '2020-03-14 02:19:20', '2020-03-14 02:34:36'),\n",
    "    (51, '2020-03-14 02:43:24', '2020-03-14 03:00:44'),\n",
    "    (52, '2020-03-14 03:17:16', '2020-03-14 03:37:04'),\n",
    "    (53, '2020-03-14 03:33:36', '2020-03-14 03:41:20'),\n",
    "    (54, '2020-03-14 03:57:16', '2020-03-14 04:18:04'),\n",
    "    (55, '2020-03-14 04:12:56', '2020-03-14 04:31:56'),\n",
    "    (56, '2020-03-14 04:27:40', '2020-03-14 04:50:20'),\n",
    "    (57, '2020-03-14 04:49:56', '2020-03-14 05:19:48'),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "formatted_date = []\n",
    "\n",
    "for entry in data:\n",
    "    entry_id, start_str, end_str = entry\n",
    "    start_dt = datetime.strptime(start_str, '%Y-%m-%d %H:%M:%S')\n",
    "    end_dt = datetime.strptime(end_str, '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    formatted_start = start_dt.strftime('%Y-%m-%dT%H:%M:%S.%f000')\n",
    "    formatted_end = end_dt.strftime('%Y-%m-%dT%H:%M:%S.%f000')\n",
    "    \n",
    "    formatted_date.append((entry_id, formatted_start, formatted_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1670673-6bfa-428c-b982-19d291b2ce45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'Open-cells-Figures-Final/t_series/Mar13-Mar14' created successfully.\n",
      "Figures in 20200313-08:12:28_20200313-08:34:48_ID1.png executed!\n",
      "Figures in 20200313-08:32:08_20200313-09:06:36_ID2.png executed!\n",
      "Figures in 20200313-09:12:48_20200313-09:31:00_ID3.png executed!\n",
      "Figures in 20200313-09:29:36_20200313-09:58:16_ID4.png executed!\n",
      "Figures in 20200313-10:00:12_20200313-10:11:56_ID5.png executed!\n",
      "Figures in 20200313-10:14:32_20200313-10:29:00_ID6.png executed!\n",
      "Figures in 20200313-10:22:40_20200313-10:46:56_ID7.png executed!\n",
      "Figures in 20200313-10:42:32_20200313-10:59:04_ID8.png executed!\n",
      "Figures in 20200313-11:00:32_20200313-11:26:20_ID9.png executed!\n",
      "Figures in 20200313-11:23:32_20200313-11:32:28_ID10.png executed!\n",
      "Figures in 20200313-11:30:44_20200313-11:51:52_ID11.png executed!\n",
      "Figures in 20200313-11:48:08_20200313-12:09:04_ID12.png executed!\n",
      "Figures in 20200313-12:03:08_20200313-12:15:00_ID13.png executed!\n",
      "Figures in 20200313-12:12:24_20200313-12:38:04_ID14.png executed!\n",
      "Figures in 20200313-12:32:48_20200313-12:56:36_ID15.png executed!\n",
      "Figures in 20200313-12:52:56_20200313-13:14:32_ID16.png executed!\n",
      "Figures in 20200313-13:53:00_20200313-14:05:16_ID17.png executed!\n",
      "Figures in 20200313-14:03:28_20200313-14:31:20_ID18.png executed!\n",
      "Figures in 20200313-15:09:36_20200313-15:19:08_ID19.png executed!\n",
      "Figures in 20200313-15:17:16_20200313-15:45:20_ID20.png executed!\n",
      "Figures in 20200313-15:54:36_20200313-15:58:08_ID21.png executed!\n",
      "Figures in 20200313-15:58:12_20200313-16:02:36_ID22.png executed!\n",
      "Figures in 20200313-15:48:00_20200313-16:12:44_ID23.png executed!\n",
      "Figures in 20200313-16:12:00_20200313-16:33:44_ID24.png executed!\n",
      "Figures in 20200313-16:29:40_20200313-16:38:20_ID25.png executed!\n",
      "Figures in 20200313-16:44:24_20200313-16:57:56_ID26.png executed!\n",
      "Figures in 20200313-16:53:20_20200313-17:13:32_ID27.png executed!\n",
      "Figures in 20200313-17:12:56_20200313-17:22:56_ID28.png executed!\n",
      "Figures in 20200313-17:13:36_20200313-17:45:48_ID29.png executed!\n",
      "Figures in 20200313-18:10:04_20200313-18:28:04_ID30.png executed!\n",
      "Figures in 20200313-18:27:28_20200313-18:50:04_ID31.png executed!\n",
      "Figures in 20200313-18:47:08_20200313-18:54:48_ID32.png executed!\n",
      "Figures in 20200313-18:54:04_20200313-18:57:08_ID33.png executed!\n",
      "Figures in 20200313-19:15:32_20200313-19:54:00_ID34.png executed!\n",
      "Figures in 20200313-19:47:12_20200313-20:15:52_ID35.png executed!\n",
      "Figures in 20200313-20:14:12_20200313-20:32:32_ID36.png executed!\n",
      "Figures in 20200313-20:26:44_20200313-20:48:12_ID37.png executed!\n",
      "Figures in 20200313-20:50:16_20200313-20:59:28_ID38.png executed!\n",
      "Figures in 20200313-20:48:36_20200313-21:19:56_ID39.png executed!\n",
      "Figures in 20200313-21:07:00_20200313-21:11:04_ID40.png executed!\n",
      "Figures in 20200313-21:16:00_20200313-21:21:40_ID41.png executed!\n",
      "Figures in 20200313-21:27:40_20200313-21:50:48_ID42.png executed!\n",
      "Figures in 20200313-21:48:20_20200313-21:59:08_ID43.png executed!\n",
      "Figures in 20200313-21:57:40_20200313-22:13:00_ID44.png executed!\n",
      "Figures in 20200313-22:28:24_20200313-22:39:36_ID45.png executed!\n",
      "Figures in 20200313-22:38:08_20200313-23:00:00_ID46.png executed!\n",
      "Figures in 20200313-23:38:56_20200314-00:16:28_ID47.png executed!\n",
      "Figures in 20200314-00:13:24_20200314-00:30:44_ID48.png executed!\n",
      "Figures in 20200314-01:06:40_20200314-01:24:08_ID49.png executed!\n",
      "Figures in 20200314-02:19:20_20200314-02:34:36_ID50.png executed!\n",
      "Figures in 20200314-02:43:24_20200314-03:00:44_ID51.png executed!\n",
      "Figures in 20200314-03:17:16_20200314-03:37:04_ID52.png executed!\n",
      "Figures in 20200314-03:33:36_20200314-03:41:20_ID53.png executed!\n",
      "Figures in 20200314-03:57:16_20200314-04:18:04_ID54.png executed!\n",
      "Figures in 20200314-04:12:56_20200314-04:31:56_ID55.png executed!\n",
      "Figures in 20200314-04:27:40_20200314-04:50:20_ID56.png executed!\n",
      "Figures in 20200314-04:49:56_20200314-05:19:48_ID57.png executed!\n",
      "ðŸ˜€ðŸ˜€ðŸ˜€ðŸ˜€IDENTIFIED ALL 57 CELLS IDENTIFIED SUCCESSFULLY!!!ðŸ˜€ðŸ˜€ðŸ˜€ðŸ˜€\n"
     ]
    }
   ],
   "source": [
    "vertical_transect(formatted_date=formatted_date, CAO_date='Mar13-Mar14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e4f7ea-b94f-4dfa-b9fb-f6efe96748cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1c7c4-177d-4a71-be71-f9db2cd328e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0fbc75-8f2c-4a67-81ef-3b60f1166392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e0c106-de35-40d0-a940-5785ef694648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c077805-4b3e-40bc-824a-dfc66c457853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f9e09d-2353-4a5c-a112-2d4780b41962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vertical-transect]",
   "language": "python",
   "name": "conda-env-vertical-transect-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
